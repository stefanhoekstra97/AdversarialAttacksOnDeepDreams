{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General library imports\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data as t_data\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "# Custom code imports\n",
    "from DataSet.lightning_cifar import CIFARDataModule\n",
    "from lightning_trainer import LitModelTrainer\n",
    "from generate_dreams.render_engine import generate_dream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global params\n",
    "\n",
    "# Set default (and available) device\n",
    "torch_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# First batch is used to generate dreams (so #batch_size). \n",
    "# subset size will be generated, inferred, saved to txt json file. \n",
    "\n",
    "# Set dream subset size and batch size:\n",
    "subset_size = 256\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# Model to use for dream gen location:\n",
    "model_loc = \"../trained_models/CIFAR/base_model/version_0/checkpoints/model-epoch=15.ckpt\"\n",
    "\n",
    "# Location of dream dataset:\n",
    "# dream_loc = '../../data/cifar/base_tanh_final_32it_1e2/'\n",
    "cifar_base_loc = '../../../data/cifar'\n",
    "\n",
    "# Transforms to use before dreaming:\n",
    "transform_dreams = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "# Load model and data(sub)set\n",
    "_ModelCheckpoint = LitModelTrainer.load_from_checkpoint(model_loc)\n",
    "\n",
    "model : nn.Module = _ModelCheckpoint.model\n",
    "\n",
    "model = model.to(torch_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_labels =  [\"airplanes\", \"cars\", \"birds\", \"cats\", \"deer\", \"dogs\", \"frogs\", \"horses\", \"ships\", \"trucks\"]\n",
    "\n",
    "# Load the two datasets as used in training:\n",
    "# _data_module = CIFARDataModule(num_workers=0, batch_size=batch_size, shuffle_data=False, )\n",
    "# _data_module.setup()\n",
    "_clean_data = torchvision.datasets.CIFAR10(root=cifar_base_loc, train=True,transform=transform_dreams, download=True  )\n",
    "\n",
    "# _clean_data = _data_module.cifar_full\n",
    "# _dream_data = _data_module.dream_dataset\n",
    "\n",
    "_data_idxs = list(range(subset_size))\n",
    "_clean_subset = t_data.Subset(_clean_data, _data_idxs)\n",
    "\n",
    "\n",
    "parallell_dataloader = DataLoader(dataset=_clean_subset, batch_size=batch_size,drop_last=True, num_workers=0, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_logits_to_df(df: pd.DataFrame, normal_logits_batch, dream_logits_batch, batch_idx, labels_batch):\n",
    "\n",
    "    for (_img_idx, normal_logits, dream_logits) in zip(range(len(normal_logits_batch)), normal_logits_batch, dream_logits_batch):\n",
    "        img_idx = _img_idx + batch_idx * batch_size\n",
    "        for (logit_idx, normal_logit, dream_logit) in zip(range(len(normal_logits)), normal_logits, dream_logits):\n",
    "\n",
    "            logit_categorical = class_labels[logit_idx]\n",
    "            \n",
    "            _row_list = []\n",
    "            _row_list.append({\n",
    "                \"img_index\": img_idx,\n",
    "                \"img_label\": labels_batch[img_idx % batch_size].item(),\n",
    "                \"logit_id\": logit_categorical,\n",
    "                'value_type': \"normal\",\n",
    "                \"logit_value\": normal_logit.item() })\n",
    "            _row_list.append({\n",
    "                \"img_index\": img_idx,\n",
    "                \"img_label\": labels_batch[img_idx % batch_size].item(),\n",
    "                \"logit_id\": logit_categorical,\n",
    "                'value_type': \"dream\",\n",
    "                \"logit_value\": dream_logit.item() })\n",
    "            _row_list.append({\n",
    "                \"img_index\": img_idx,\n",
    "                \"img_label\": labels_batch[img_idx % batch_size].item(),\n",
    "                \"logit_id\": logit_categorical,\n",
    "                'value_type': \"diff\",\n",
    "                \"logit_value\": dream_logit.item() - normal_logit.item() })\n",
    "\n",
    "            logit_row_df = pd.DataFrame(data=_row_list)\n",
    "            df = pd.concat([df, logit_row_df], axis=0, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean change: 0.04646193981170654, max change: 0.3159600794315338, min change: 0.0\n",
      "it 32, loss: -32.59113311767578, average l2 other targets: 430.7247009277344, weight factor of L2 dist: 0.0\n",
      "mean change: 0.048030786216259, max change: 0.3094111382961273, min change: 0.0\n",
      "it 32, loss: -35.06561279296875, average l2 other targets: 434.37506103515625, weight factor of L2 dist: 0.0\n",
      "mean change: 0.047301582992076874, max change: 0.3106195628643036, min change: 0.0\n",
      "it 32, loss: -33.59252166748047, average l2 other targets: 434.1109924316406, weight factor of L2 dist: 0.0\n",
      "mean change: 0.0476551316678524, max change: 0.3114229440689087, min change: 0.0\n",
      "it 32, loss: -35.602996826171875, average l2 other targets: 410.9549560546875, weight factor of L2 dist: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "logit_dataframe = pd.DataFrame(columns=[\"img_index\", \"img_label\", \"logit_id\", \"value_type\", \"logit_value\"])\n",
    "optimizer_l = lambda p: torch.optim.Adam(p, lr=1e-2)\n",
    "\n",
    "for batch_idx,batch in enumerate(parallell_dataloader):\n",
    "    x, y = batch\n",
    "\n",
    "    x, y = x.to(torch_device), y.to(torch_device)\n",
    "    x_dream = generate_dream(optimizer_l=optimizer_l, iterations=32, eps_limit=1, l2_weight=0.0, verbose=True, model=model, batch=batch)\n",
    "    with torch.no_grad():\n",
    "        orig_logits = model(x)\n",
    "        dream_logits = model(x_dream)\n",
    "\n",
    "    logit_dataframe = batch_logits_to_df(logit_dataframe, orig_logits, dream_logits, batch_idx, y )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_dataframe.to_pickle('32it_lr1e-2.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6bbff785f7dca992236909a4969e32f59102c4fceccb1043997c81bace8c71c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
